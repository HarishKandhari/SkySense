# build_index.py

import os
import pickle
import numpy as np
import faiss

from utils import load_sample_docs
from retriever import get_embedding

# â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
CHUNK_SIZE    = 2000
CHUNK_OVERLAP = 200
OUTPUT_PATH   = "data/vector_index.pkl"

def chunk_text(text: str, size: int, overlap: int) -> list[str]:
    """Split `text` into chunks of `size` characters with `overlap`."""
    chunks = []
    start = 0
    length = len(text)
    while start < length:
        end = min(start + size, length)
        chunks.append(text[start:end])
        start += size - overlap
    return chunks

if __name__ == "__main__":
    print("ðŸ‘‹ Hello from build_index.py")

    # 1ï¸âƒ£ Load documents (we expect just one .txt)
    print("ðŸ“„ Loading document(s)â€¦")
    docs = load_sample_docs("data/sample_docs")
    if not docs:
        print("âš ï¸ No documents found in data/sample_docs/")
        exit(1)

    # Weâ€™ll concatenate them if thereâ€™s more than one, but typically itâ€™s 1 file.
    full_text = "\n\n".join(docs)
    print(f"ðŸ“„ Loaded document of length {len(full_text)} characters.")

    # 2ï¸âƒ£ Chunk the single document
    print("âœ‚ï¸  Step 1: Chunking the single document into piecesâ€¦")
    chunks = chunk_text(full_text, CHUNK_SIZE, CHUNK_OVERLAP)
    print(f"ðŸ“„ Created {len(chunks)} chunk(s).")

    # 3ï¸âƒ£ Embed each chunk
    print("ðŸ” Step 2: Creating embeddings for each chunkâ€¦")
    vectors = [
        np.array(get_embedding(chunk), dtype="float32")
        for chunk in chunks
    ]
    print("âœ… Embeddings generated.")

    # 4ï¸âƒ£ Build a FAISS index
    dim = vectors[0].shape[0]
    index = faiss.IndexFlatL2(dim)
    index.add(np.stack(vectors))
    print(f"âš™ï¸ FAISS index created with chunked data.")

    # 5ï¸âƒ£ Save the index + chunk texts
    print(f"ðŸ’¾ Saving index + chunks to {OUTPUT_PATH}â€¦")
    with open(OUTPUT_PATH, "wb") as f:
        # we save a tuple of (index, chunks)
        pickle.dump((index, chunks), f)
    print("ðŸŽ‰ All done! Index saved successfully.")